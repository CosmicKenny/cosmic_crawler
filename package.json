{
  "name": "node_crawler",
  "version": "1.0.0",
  "description": "A crawler that churn out all the links and save the HTML into static file into local machine. Just like HTTrack, but better.",
  "main": "index.js",
  "scripts": {
    "test": "echo \"Error: no test specified\" && exit 1"
  },
  "keywords": [
    "crawl",
    "html"
  ],
  "author": "cosmiccommand",
  "license": "MIT",
  "dependencies": {
    "chalk": "^2.4.2",
    "crawler": "^1.2.0",
    "fs": "0.0.1-security",
    "html-validator": "^4.0.2",
    "https": "^1.0.0",
    "json2csv": "^4.4.1",
    "node-fetch": "^2.6.0",
    "pa11y": "^5.1.0",
    "pa11y-reporter-html": "^1.0.0",
    "puppeteer": "^1.14.0",
    "queue": "^6.0.1",
    "request": "^2.88.0",
    "sanitize-html": "^1.20.1",
    "simplecrawler": "^1.1.6",
    "util": "^0.12.0"
  }
}
