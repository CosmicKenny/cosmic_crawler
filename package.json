{
  "name": "node_crawler",
  "version": "1.0.0",
  "description": "A crawler that churn out all the links and save the HTML into static file into local machine. Just like HTTrack, but better.",
  "main": "index.js",
  "scripts": {
    "test": "echo \"Error: no test specified\" && exit 1"
  },
  "keywords": [
    "crawl",
    "html"
  ],
  "author": "cosmiccommand",
  "license": "MIT",
  "dependencies": {
    "crawler": "^1.2.0",
    "fs": "0.0.1-security",
    "https": "^1.0.0",
    "json2csv": "^4.4.1",
    "pa11y": "^5.1.0",
    "puppeteer": "^1.14.0",
    "queue": "^6.0.1",
    "simplecrawler": "^1.1.6"
  }
}
